{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging Chemical Named Entities with ddlite\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As with the gene tagging example, this task is broken down into several steps:\n",
    "\n",
    "1. Obtain and parse input data (ChemDNER Corpus PubMed abstracts)\n",
    "2. Extract candidates for tagging\n",
    "3. Generate features\n",
    "4. Write distant supervision rules\n",
    "5. Learn the tagging model\n",
    "\n",
    "## 1. Obtaining Data\n",
    "\n",
    "**ChemDNER Corpus v1.0**\n",
    "\n",
    "The ChemDNER corpus is used to evaluate systems for identifying chemical names in biomedical literature. The corpus consists of 10,000 PubMED abstracts and their corresponding label sets of named chemical entities. The data is broken down as follows:\n",
    "\n",
    "* 3500 Training\n",
    "* 3500 Development\n",
    "* 3000 Evaluation (Testing)\n",
    "\n",
    "This data set is [publicly available](http://www.biocreative.org/resources/biocreative-iv/chemdner-corpus/) and can be downloaded directly using the shell script below. This will download and extact all files into your dd-biolib datasets directory.\n",
    "\n",
    "    load_data.sh\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting Candidate Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import umls\n",
    "from ddlite import *\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data, we need to preprocess our documents. The SentenceParser class is a wrapper for CoreNLP and parses documents into tagged sentences (i.e., words, lemmas, POS tags, dependency parents, and dependency labels). This takes a bit of time, so the ChemdnerCorpus object will cache parsed files to disk after a document is accessed for the first time.  \n",
    "\n",
    "**Note**: This is a standard preprocessing step for all DeepDive applications. Theoretically, you could swap out the CoreNLP parser for something else here if you wished, e.g., [spacy.io](https://spacy.io) (faster, less accurate), [cTAKES](http://ctakes.apache.org) or [NLP4J](https://github.com/emorynlp/nlp4j) (for clinical text tagging).\n",
    "\n",
    "The ChemDNER corpus defines cross-validation sets, so let's create a dictionary of the first 10 training documents as well as a list of all true chemical entity mentions in those documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 true chemical entity mentions\n"
     ]
    }
   ],
   "source": [
    "parser = SentenceParser(absolute_path=True)\n",
    "corpus = ChemdnerCorpus('datasets/chemdner_corpus/', parser=None)\n",
    "\n",
    "# load the first 10 training documents and collapse all sentences into a single list\n",
    "pmids = [pmid for pmid in corpus.cv[\"training\"].keys()[:10]]\n",
    "documents = {pmid:corpus[pmid][\"sentences\"] for pmid in pmids}\n",
    "sentences = reduce(lambda x,y:x+y, documents.values())\n",
    "\n",
    "# load gold annotation tags\n",
    "annotations = [corpus.annotations[pmid] for pmid in pmids if pmid in corpus.annotations]\n",
    "annotations = reduce(lambda x,y:x+y, annotations)\n",
    "annotations = [a.text for a in annotations]\n",
    "print(\"%d true chemical entity mentions\" % len(annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Matching\n",
    "\n",
    "The easiest way to identify candidates is through simple string matching using a dictionary of known entity names. Curating good lexicons can take some time, so we use pre-existing dictionaries provided by the *tmChem* tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 167 candidate entities\n",
      "Recall: 0.65\n"
     ]
    }
   ],
   "source": [
    "# dictionaries from tmChem\n",
    "dict_fnames = [\"datasets/dictionaries/chemdner/mention_chemical.txt\",\n",
    "              \"datasets/dictionaries/chemdner/chebi.txt\",\n",
    "              \"datasets/dictionaries/chemdner/addition.txt\"]\n",
    "chemicals = []\n",
    "for fname in dict_fnames:\n",
    "    chemicals += [line.split(\"\\t\")[0] for line in open(fname,\"rU\").readlines()]\n",
    "\n",
    "extractor = DictionaryMatch('C', chemicals, ignore_case=True)\n",
    "\n",
    "candidates = Entities(extractor, sentences)\n",
    "mentions1 = [\" \".join([e.words[i] for i in e.idxs]) for e in candidates.entities]\n",
    "\n",
    "# Crude/incorrect estimate of how well we did (ignores actual span match)\n",
    "m = len([term for term in annotations if term in mentions1])\n",
    "\n",
    "print(\"Found %d candidate entities\" % len(candidates.entities))\n",
    "print(\"Recall: %.2f\" % (float(m) / len(annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMLS Matching\n",
    "Straight dictionary matching doesn't actually find that many entities in our small subset. Rather than using existing dictionary files, we can interface directly with the UMLS and finding string matches by semantic type.  Note that this approach only uses ontologies curated by the UMLS, which we specify using the source_vocab paramter. By default, UmlsMatch uses all curated ontologies, which results in large candidate entity sets. Here we restrict to 3 ontologies that provide reasonable coverage: RxNorm, SNOMED CT, and MeSH (Medical Subject Headings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 212 candidate entities\n",
      "Recall: 0.65\n"
     ]
    }
   ],
   "source": [
    "extractor = umls.UmlsMatch('C', semantic_types=[\"Substance\"], \n",
    "                           source_vocab=[\"RXNORM\",\"SNOMEDCT_US\",\"MSH\"], ignore_case=True)\n",
    "\n",
    "candidates = Entities(extractor, sentences)\n",
    "mentions2 = [\" \".join([e.words[i] for i in e.idxs]) for e in candidates.entities]\n",
    "\n",
    "m = len([term for term in annotations if term in mentions2])\n",
    "\n",
    "print(\"Found %d candidate entities\" % len(candidates.entities))\n",
    "print(\"Recall: %.2f\" % (float(m) / len(annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 379 candidate entities\n",
      "Recall: 0.68\n"
     ]
    }
   ],
   "source": [
    "# Merging candidates doesn't really lead to much improvement\n",
    "mentions = mentions1 + mentions2\n",
    "m = len([term for term in annotations if term in mentions])\n",
    "print(\"Found %d candidate entities\" % len(mentions))\n",
    "print(\"Recall: %.2f\" % (float(m) / len(annotations)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "Recall isn't that great and merging candidates doesn't provide much performance boost. If we look at the gold standard annotations, we can see why our string matching misses a some set of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAD : 1\n",
      "(1)H : 1\n",
      "(31)P : 1\n",
      "C2H3OH : 1\n",
      "hydroxy : 4\n",
      "UVI2008 : 1\n",
      "Hydroxy : 1\n",
      "4-methyl : 1\n",
      "carboxyl : 1\n",
      "chloroquine : 1\n",
      "cholesterol : 2\n",
      "l-amino acid : 2\n",
      "mallotophenone : 1\n",
      "organochlorine : 3\n",
      "carbonyl di-imidazole : 1\n",
      "dimeric phloroglucinols : 2\n",
      "mallotojaponins B (1) and C (2) : 1\n",
      "(22R)-hydroxylanosta-7,9(11),24-trien-3-one : 1\n",
      "6,8-dihydroxy-3-methyl-3,4-dihydroisocoumarin : 1\n"
     ]
    }
   ],
   "source": [
    "missed = [term for term in annotations if term not in mentions]\n",
    "missed = {term:missed.count(term) for term in missed}\n",
    "for term in sorted(missed,key=len):\n",
    "    print(\"%s : %d\" % (term,missed[term]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing Problems** CoreNLP isn't trained for biomedical text (by default), so for entities containing hyphens or paranthesis, tokenization by broken in complicated chemical names. \n",
    "\n",
    "* 6,8-dihydroxy-3-methyl-3,4-dihydroisocoumarin\n",
    "* 6,8-dihydroxy-3-methyl-3 ,4 - dihydroisocoumarin\n",
    "\n",
    "* (22R)-hydroxylanosta-7,9(11),24-trien-3-one\n",
    "* ( 22R ) - hydroxylanosta-7 ,9 ( 11 ) ,24 - trien-3-one\n",
    "\n",
    "*tmChem* addresses this issue using regular expressions on the original (unparsed) text. To address this, a RegexMatch class will be implemented shortly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generating Features\n",
    "After we get our recall as high as possible, we need to generate features for each mention so that we can idenfiy true and negative entity instances. In ddlite, this is very simple and automated -- remember the goal of ddlite is rapid prototyping of distance supervision rules not feature engineering!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 6627 features for each of 212 mentions\n"
     ]
    }
   ],
   "source": [
    "candidates.extract_features()\n",
    "print \"Extracted {} features for each of {} mentions\".format(*candidates.feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distant Supervision Rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Learning the tagging model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Best-in-class Tagger\n",
    "\n",
    "The winning system in the 2013 BioCreative IV CHEMDNER task was tmChem which used 2 linear chain conditional random fields (CRF) with different tokenziation approaches and feature sets.\n",
    "\n",
    "| Model       | Precision | Recall | F1     |\n",
    "|-------------|-----------|--------|--------|\n",
    "| Model 1     | 0.8595    | 0.8721 | 0.8657 |\n",
    "| Model 2     | **0.8909**    | 0.8575 | **0.8739** |\n",
    "| Heuristic Combination     | 0.8516    | 0.8906 | 0.8706 |\n",
    "| Highest Recall | 0.7672    | **0.9212** | 0.8372 |\n",
    "\n",
    "Leaman, Robert, Chih-Hsuan Wei, and Zhiyong Lu. [\"tmChem: a high performance approach for chemical named entity recognition and normalization.\"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4331693/) J. Cheminformatics 7.S-1 (2015): S3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
