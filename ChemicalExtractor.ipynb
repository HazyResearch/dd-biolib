{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Extracting Chemical Named Entities\n",
    "\n",
    "## 1. Obtaining Data\n",
    "\n",
    "**ChemDNER Corpus v1.0**\n",
    "\n",
    "The ChemDNER corpus consists of 10,000 PubMED abstracts and their corresponding label sets of named chemical entities. This data set is [publicly available](http://www.biocreative.org/resources/biocreative-iv/chemdner-corpus/) and can be downloaded directly using the shell script \n",
    "\n",
    "    load_data.sh\n",
    "\n",
    "## 2. Loading Documents\n",
    "For the extraction step, our goal is to get the highest recall as possible. In cases where we have labeled data, it's easy to get a recall estimate for our extraction pipline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import umls\n",
    "import codecs\n",
    "import operator\n",
    "import itertools\n",
    "from ddlite import *\n",
    "from datasets import *\n",
    "from utils import unescape_penn_treebank\n",
    "from lexicons import AllUpperNounsMatcher,RuleTokenizedDictionaryMatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250 PubMed abstracts\n",
      "1854 ChemNDER gold entities\n",
      "56361 tokens\n"
     ]
    }
   ],
   "source": [
    "parser = SentenceParser()\n",
    "corpus = ChemdnerCorpus('datasets/chemdner_corpus/', parser=parser, \n",
    "                        cache_path=\"examples/cache/chemdner/\")\n",
    "\n",
    "# ChemNDER has pre-defined cross-validation folds -- use 25 docs\n",
    "dev_set = sorted(corpus.cv[\"development\"].keys())[:250]\n",
    "\n",
    "# load training documents and collapse all sentences into a single list\n",
    "documents = {doc_id:(corpus[doc_id][\"sentences\"],corpus[doc_id][\"tags\"]) for doc_id in dev_set}\n",
    "sentences, gold_entities = zip(*documents.values())\n",
    "sentences = list(itertools.chain.from_iterable(sentences))\n",
    "gold_entities = list(itertools.chain.from_iterable(gold_entities))\n",
    "\n",
    "# summary statistics\n",
    "gold_entity_n = len(list(itertools.chain.from_iterable(gold_entities)))\n",
    "word_n = sum([len(sent.words) for sent in sentences])\n",
    "print(\"%d PubMed abstracts\" % len(documents))\n",
    "print(\"%d ChemNDER gold entities\" % gold_entity_n)\n",
    "print(\"%d tokens\" % word_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Matchers\n",
    "\n",
    "The easiest way to identify candidates is through simple string matching using a dictionary of known entity names. Curating good lexicons can take some time, so we use pre-existing dictionaries provided by the *tmChem* tagger and a UMLS dictionary of all *Substance* semantic types (see the UMLS notebook for instructions how to create arbitrary dictionaries). The goal of matching is to acheive as high recall as possible. In real-world applications, we can't compute true recall, so it's important to try and get good coverage. \n",
    "\n",
    "- **DictionaryMatch** Match to an existing dictionary of known entity names.\n",
    "\n",
    "- **RegexMatcher** Match words according to simple regular expressions. Here we just match Greek letters and  simple patterns of the form -3,4- which tend to indicate chemical names.\n",
    "\n",
    "- **RuleTokenizedDictionaryMatch** Match a dictionary under a different tokenization scheme (in this case we provide a whitespace tokenizer. The resulting labels are mapped back into our primary CoreNLP token offset space.\n",
    "\n",
    "- **AllUpperNounsMatcher** (From the Gene Tagger example) Identify all uppercase nouns in text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenizer for matching within raw sentence text\n",
    "def rule_tokenizer(s):\n",
    "    s = re.sub(\"([,?!:;] )\",r\" \\1\",s)\n",
    "    s = re.sub(\"([.]$)\",r\" .\",s)\n",
    "    return s.split()\n",
    "\n",
    "# load dictionaries \n",
    "dict_fnames = [\"datasets/dictionaries/chemdner/mention_chemical.txt\",\n",
    "               \"datasets/dictionaries/chemdner/chebi.txt\",\n",
    "               \"datasets/dictionaries/chemdner/addition.txt\",\n",
    "               \"datasets/dictionaries/umls/substance-sab-all.txt\",\n",
    "               \"datasets/dictionaries/chemdner/train.chemdner.vocab.txt\"]\n",
    "\n",
    "chemicals = []\n",
    "for fname in dict_fnames:\n",
    "    chemicals += [line.strip().split(\"\\t\")[0] for line in codecs.open(fname,\"rU\",\"utf-8\").readlines()]\n",
    "\n",
    "# remove stopwords\n",
    "fname = \"datasets/dictionaries/chemdner/stopwords.txt\"\n",
    "stopwords = {line.strip().split(\"\\t\")[0]:1 for line in open(fname,\"rU\").readlines()}\n",
    "chemicals = {term:1 for term in chemicals if term not in stopwords}.keys()\n",
    "\n",
    "# create matchers and extract candidates\n",
    "extr1 = DictionaryMatch('C', chemicals, ignore_case=True)\n",
    "extr2 = RuleTokenizedDictionaryMatch('C', chemicals, ignore_case=True, tokenizer=rule_tokenizer)\n",
    "extr3 = RegexMatch('C',\"[αβΓγΔδεϝζηΘθικΛλμνΞξοΠπρΣστυΦφχΨψΩω]+[-]+[A-Za-z]+\", ignore_case=True)\n",
    "extr4 = RegexMatch('C', \"([-]*(\\d[,]*)+[-])\", ignore_case=True)\n",
    "extr5 = RegexMatch('C',\"[αβΔδη]+\", ignore_case=True)\n",
    "extr6 = AllUpperNounsMatcher('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matcher = MultiMatcher(extr1, extr2, extr3, extr4, extr5, extr6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting Candidate Entities\n",
    "Once we have matchers, we want to generate and store our candidate entity set for later use in learning. (Note this can take a long time, which is why you should precompute candidates before moving to the learning stage.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9388 candidate entities\n",
      "Candidates: 16.66% of all tokens\n",
      "Annotations 3.29% of all tokens\n",
      "~recall: 0.73 (1359/1854)\n"
     ]
    }
   ],
   "source": [
    "candidates = Entities(sentences, matcher)\n",
    "\n",
    "# Crude recall estimate (ignores actual span match)\n",
    "mentions = [\" \".join(unescape_penn_treebank([e.words[i] for i in e.idxs])) for e in candidates]\n",
    "gold_mentions = list(zip(*itertools.chain.from_iterable(gold_entities))[0])\n",
    "\n",
    "for m in mentions:\n",
    "    if m in gold_mentions:\n",
    "        gold_mentions.remove(m)\n",
    "tp = gold_entity_n - len(gold_mentions)\n",
    "\n",
    "print(\"Found %d candidate entities\" % len(candidates))\n",
    "print(\"Candidates: %.2f%% of all tokens\" % (len(candidates)/float(word_n) * 100))\n",
    "print(\"Annotations %.2f%% of all tokens\" % (gold_entity_n/float(word_n) * 100))\n",
    "print(\"~recall: %.2f (%d/%d)\" % (float(tp) / gold_entity_n, tp, gold_entity_n))\n",
    "\n",
    "candidates.dump_candidates(\"examples/cache/chem-candidates.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".node {\n",
       "  cursor: pointer;\n",
       "}\n",
       "\n",
       ".node circle {\n",
       "  fill: #fff;\n",
       "  stroke: steelblue;\n",
       "  stroke-width: 3px;\n",
       "}\n",
       "\n",
       ".node text {\n",
       "  font: 12px sans-serif;\n",
       "}\n",
       "\n",
       ".edge {\n",
       "  fill: none;\n",
       "  stroke: #ccc;\n",
       "  stroke-width: 2px;\n",
       "  cursor: pointer;\n",
       "}\n",
       "\n",
       ".highlight {\n",
       "  stroke: red;\n",
       "  stroke-width: 3px;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<!--Provide the canvas id (twice) and the words via python string formatting here--!>\n",
       "<div id=\"tree-chart-6246832077160690717\"></div>\n",
       "<div id=\"raw-seq-6246832077160690717\">\n",
       "<span class=\"word-6246832077160690717-0\">Understanding</span> <span class=\"word-6246832077160690717-1\">these</span> <span class=\"word-6246832077160690717-2\">long-range</span> <span class=\"word-6246832077160690717-3\">amino</span> <span class=\"word-6246832077160690717-4\">acid</span> <span class=\"word-6246832077160690717-5\">networks</span> <span class=\"word-6246832077160690717-6\">in</span> <span class=\"word-6246832077160690717-7\">TS</span> <span class=\"word-6246832077160690717-8\">thus</span> <span class=\"word-6246832077160690717-9\">gives</span> <span class=\"word-6246832077160690717-10\">insight</span> <span class=\"word-6246832077160690717-11\">into</span> <span class=\"word-6246832077160690717-12\">the</span> <span class=\"word-6246832077160690717-13\">coordination</span> <span class=\"word-6246832077160690717-14\">of</span> <span class=\"word-6246832077160690717-15\">the</span> <span class=\"word-6246832077160690717-16\">two</span> <span class=\"word-6246832077160690717-17\">active</span> <span class=\"word-6246832077160690717-18\">sites</span> <span class=\"word-6246832077160690717-19\">within</span> <span class=\"word-6246832077160690717-20\">TS</span> <span class=\"word-6246832077160690717-21\">.</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "$.getScript(\"http://d3js.org/d3.v3.min.js\", function () {\n",
       "// See http://bl.ocks.org/d3noob/8375092\n",
       "// Three vars need to be provided via python string formatting:\n",
       "var chartId = \"6246832077160690717\";\n",
       "var root = {\"attrib\": {\"token_idx\": \"263\", \"word\": \"Understanding\", \"dep_label\": \"ROOT\", \"pos\": \"VBG\", \"lemma\": \"understand\", \"word_idx\": \"0\", \"dep_parent\": \"0\"}, \"children\": [{\"attrib\": {\"token_idx\": \"305\", \"word\": \"networks\", \"dep_label\": \"dobj\", \"pos\": \"NNS\", \"lemma\": \"network\", \"word_idx\": \"5\", \"dep_parent\": \"1\"}, \"children\": [{\"attrib\": {\"token_idx\": \"277\", \"word\": \"these\", \"dep_label\": \"det\", \"pos\": \"DT\", \"lemma\": \"these\", \"word_idx\": \"1\", \"dep_parent\": \"6\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"283\", \"word\": \"long-range\", \"dep_label\": \"amod\", \"pos\": \"JJ\", \"lemma\": \"long-range\", \"word_idx\": \"2\", \"dep_parent\": \"6\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"294\", \"word\": \"amino\", \"dep_label\": \"compound\", \"pos\": \"NN\", \"lemma\": \"amino\", \"word_idx\": \"3\", \"dep_parent\": \"6\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"300\", \"word\": \"acid\", \"dep_label\": \"compound\", \"pos\": \"NN\", \"lemma\": \"acid\", \"word_idx\": \"4\", \"dep_parent\": \"6\"}, \"children\": []}]}, {\"attrib\": {\"token_idx\": \"317\", \"word\": \"TS\", \"dep_label\": \"nmod\", \"pos\": \"NN\", \"lemma\": \"ts\", \"word_idx\": \"7\", \"dep_parent\": \"1\"}, \"children\": [{\"attrib\": {\"token_idx\": \"314\", \"word\": \"in\", \"dep_label\": \"case\", \"pos\": \"IN\", \"lemma\": \"in\", \"word_idx\": \"6\", \"dep_parent\": \"8\"}, \"children\": []}]}, {\"attrib\": {\"token_idx\": \"325\", \"word\": \"gives\", \"dep_label\": \"dep\", \"pos\": \"VBZ\", \"lemma\": \"give\", \"word_idx\": \"9\", \"dep_parent\": \"1\"}, \"children\": [{\"attrib\": {\"token_idx\": \"320\", \"word\": \"thus\", \"dep_label\": \"advmod\", \"pos\": \"RB\", \"lemma\": \"thus\", \"word_idx\": \"8\", \"dep_parent\": \"10\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"331\", \"word\": \"insight\", \"dep_label\": \"dobj\", \"pos\": \"NN\", \"lemma\": \"insight\", \"word_idx\": \"10\", \"dep_parent\": \"10\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"348\", \"word\": \"coordination\", \"dep_label\": \"nmod\", \"pos\": \"NN\", \"lemma\": \"coordination\", \"word_idx\": \"13\", \"dep_parent\": \"10\"}, \"children\": [{\"attrib\": {\"token_idx\": \"339\", \"word\": \"into\", \"dep_label\": \"case\", \"pos\": \"IN\", \"lemma\": \"into\", \"word_idx\": \"11\", \"dep_parent\": \"14\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"344\", \"word\": \"the\", \"dep_label\": \"det\", \"pos\": \"DT\", \"lemma\": \"the\", \"word_idx\": \"12\", \"dep_parent\": \"14\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"379\", \"word\": \"sites\", \"dep_label\": \"nmod\", \"pos\": \"NNS\", \"lemma\": \"site\", \"word_idx\": \"18\", \"dep_parent\": \"14\"}, \"children\": [{\"attrib\": {\"token_idx\": \"361\", \"word\": \"of\", \"dep_label\": \"case\", \"pos\": \"IN\", \"lemma\": \"of\", \"word_idx\": \"14\", \"dep_parent\": \"19\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"364\", \"word\": \"the\", \"dep_label\": \"det\", \"pos\": \"DT\", \"lemma\": \"the\", \"word_idx\": \"15\", \"dep_parent\": \"19\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"368\", \"word\": \"two\", \"dep_label\": \"nummod\", \"pos\": \"CD\", \"lemma\": \"two\", \"word_idx\": \"16\", \"dep_parent\": \"19\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"372\", \"word\": \"active\", \"dep_label\": \"amod\", \"pos\": \"JJ\", \"lemma\": \"active\", \"word_idx\": \"17\", \"dep_parent\": \"19\"}, \"children\": []}, {\"attrib\": {\"token_idx\": \"392\", \"word\": \"TS\", \"dep_label\": \"nmod\", \"pos\": \"NN\", \"lemma\": \"ts\", \"word_idx\": \"20\", \"dep_parent\": \"19\"}, \"children\": [{\"attrib\": {\"token_idx\": \"385\", \"word\": \"within\", \"dep_label\": \"case\", \"pos\": \"IN\", \"lemma\": \"within\", \"word_idx\": \"19\", \"dep_parent\": \"21\"}, \"children\": []}]}]}]}]}, {\"attrib\": {\"token_idx\": \"394\", \"word\": \".\", \"dep_label\": \"punct\", \"pos\": \".\", \"lemma\": \".\", \"word_idx\": \"21\", \"dep_parent\": \"1\"}, \"children\": []}]};\n",
       "var highlightIdxs = [[3]];\n",
       "\n",
       "// Highlight words / nodes\n",
       "var COLORS = [\"#ff5c33\", \"#ffcc00\", \"#33cc33\", \"#3399ff\"];\n",
       "function highlightWords() {\n",
       "  for (var i=0; i < highlightIdxs.length; i++) {\n",
       "    var c = COLORS[i];\n",
       "    var idxs = highlightIdxs[i];\n",
       "    for (var j=0; j < idxs.length; j++) {\n",
       "      d3.selectAll(\".word-\"+chartId+\"-\"+idxs[j]).style(\"stroke\", c).style(\"background\", c);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "// Constants\n",
       "var margin = {top: 20, right: 20, bottom: 20, left: 20},\n",
       "width = 800 - margin.left - margin.right,\n",
       "height = 350 - margin.top - margin.bottom,\n",
       "R = 5;\n",
       "\n",
       "// Create the d3 tree object\n",
       "var tree = d3.layout.tree()\n",
       "  .size([width, height]);\n",
       "\n",
       "// Create the svg canvas\n",
       "var svg = d3.select(\"#tree-chart-\" + chartId)\n",
       "  .append(\"svg\")\n",
       "  .attr(\"width\", width + margin.left + margin.right)\n",
       "  .attr(\"height\", height + margin.top + margin.bottom)\n",
       "  .append(\"g\")\n",
       "  .attr(\"transform\", \"translate(\" + margin.left + \",\" + margin.top + \")\");\n",
       "\n",
       "function renderTree() {\n",
       "  var nodes = tree.nodes(root),\n",
       "  edges = tree.links(nodes);\n",
       "\n",
       "  // Place the nodes\n",
       "  var nodeGroups = svg.selectAll(\"g.node\")\n",
       "    .data(nodes)\n",
       "    .enter().append(\"g\")\n",
       "    .attr(\"class\", \"node\")\n",
       "    .attr(\"transform\", function(d) { return \"translate(\" + d.x + \",\" + d.y + \")\"; });\n",
       "       \n",
       "  // Append circles\n",
       "  nodeGroups.append(\"circle\")\n",
       "    //.on(\"click\", function() {\n",
       "    //  d3.select(this).classed(\"highlight\", !d3.select(this).classed(\"highlight\")); })\n",
       "    .attr(\"r\", R)\n",
       "    .attr(\"class\", function(d) { return \"word-\"+chartId+\"-\"+d.attrib.word_idx; });\n",
       "     \n",
       "  // Append the actual word\n",
       "  nodeGroups.append(\"text\")\n",
       "    .text(function(d) { return d.attrib.word; })\n",
       "    .attr(\"text-anchor\", function(d) { \n",
       "      return d.children && d.children.length > 0 ? \"start\" : \"middle\"; })\n",
       "    .attr(\"dx\", function(d) { \n",
       "      return d.children && d.children.length > 0 ? R + 3 : 0; })\n",
       "    .attr(\"dy\", function(d) { \n",
       "      return d.children && d.children.length > 0 ? 0 : 3*R + 3; });\n",
       "\n",
       "  // Place the edges\n",
       "  var edgePaths = svg.selectAll(\"path\")\n",
       "    .data(edges)\n",
       "    .enter().append(\"path\")\n",
       "    .attr(\"class\", \"edge\")\n",
       "    .on(\"click\", function() {\n",
       "      d3.select(this).classed(\"highlight\", !d3.select(this).classed(\"highlight\")); })\n",
       "    .attr(\"d\", d3.svg.diagonal());\n",
       "}\n",
       "\n",
       "renderTree();\n",
       "highlightWords();\n",
       "});\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "candidates[40].render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "For distant supervision, we want our candidate set to have high recall. We currently fall short for chemical named entities. If we look at the gold standard annotations, we can see why our string matching misses some entities. Note how paranthesis and other tokenization issues result in many missed entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est. Tokenization Errors: 250\n",
      "Est. Out-of-vocabulary Errors: 245\n"
     ]
    }
   ],
   "source": [
    "# What are we missing due to tokenization errors?\n",
    "regexes = [re.compile(\"[αβΓγΔδεϝζηΘθικΛλμνΞξοΠπρΣστυΦφχΨψΩω]+[-]+[A-Za-z]+\")]\n",
    "regexes += [re.compile(\"([-]*(\\d[,]*)+[-])\")]\n",
    "regexes += [re.compile(\"[αβΔδη]+\")]\n",
    "\n",
    "# regular expression matches\n",
    "def regex_match(t):\n",
    "    for regex in regexes:\n",
    "        if regex.search(t):\n",
    "            return True\n",
    "    return False\n",
    "            \n",
    "tokenization_errors = [term for term in gold_mentions if term in chemicals or regex_match(term)]\n",
    "tokenization_errors = {term:tokenization_errors.count(term) for term in tokenization_errors}\n",
    "oov_errors = [term for term in gold_mentions if term not in tokenization_errors]\n",
    "oov_errors = {term:oov_errors.count(term) for term in oov_errors}\n",
    "\n",
    "print(\"Est. Tokenization Errors: %d\" % (sum(tokenization_errors.values())))\n",
    "print(\"Est. Out-of-vocabulary Errors: %d\" % (sum(oov_errors.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that almost half our errors stem from tokenization issues. If we fixed all of those errors, we would have ~0.87 recall on the development set. If we actually look at OOV mentions we missed, we see there is considerable room for refining regular expressions to identify mentions like NaAsO(2) or FeSe, which just consist of element names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metachromins L-Q: 3\n",
      "coumarin glycosides: 2\n",
      "sesquiterpenoid quinones: 2\n",
      "LixC6: 2\n",
      "CuZn: 1\n",
      "sesquiterpenoid mycotoxins: 1\n",
      "nitensosides A-B: 1\n",
      "Li4+xTi5O12: 1\n",
      "Li4Ti5O12: 1\n",
      "ortho-dihydroxy: 1\n",
      "Vulcan XC-72R: 1\n",
      "NaBH(4): 1\n",
      "Sn(2+): 1\n",
      "acylated flavone glycoside: 1\n",
      "d-calycanthine: 1\n",
      "xeroboside: 1\n",
      "LiPF6: 1\n",
      "Threonine: 1\n",
      "LixFePO4: 1\n",
      "calycanthoside: 1\n"
     ]
    }
   ],
   "source": [
    "# print out our out of vocabulary terms\n",
    "for term in sorted(oov_errors.items(),key=operator.itemgetter(1),reverse=1):\n",
    "    print(\"%s: %d\" % (term[0], oov_errors[term[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Best-in-class Tagger\n",
    "\n",
    "The winning system in the 2013 BioCreative IV CHEMDNER task was tmChem which used 2 linear chain conditional random fields (CRF) with different tokenziation approaches and feature sets.\n",
    "\n",
    "| Model       | Precision | Recall | F1     |\n",
    "|-------------|-----------|--------|--------|\n",
    "| Model 1     | 0.8595    | 0.8721 | 0.8657 |\n",
    "| Model 2     | **0.8909**    | 0.8575 | **0.8739** |\n",
    "| Heuristic Combination     | 0.8516    | 0.8906 | 0.8706 |\n",
    "| Highest Recall | 0.7672    | **0.9212** | 0.8372 |\n",
    "\n",
    "Leaman, Robert, Chih-Hsuan Wei, and Zhiyong Lu. [\"tmChem: a high performance approach for chemical named entity recognition and normalization.\"](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4331693/) J. Cheminformatics 7.S-1 (2015): S3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
